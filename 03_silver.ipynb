{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0fe0c2-457c-4836-9b62-be3fbdfe4c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação de Database\n",
    "def create_database(nome_db):\n",
    "    \"\"\"\n",
    "    Criação do Database, instanciado no notebook Entrypoints\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {nome_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae384bc-b70b-4fff-8c9b-755ab64b25dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_silver_calendar(path_bronze_calendar, path_silver_calendar):\n",
    "    \"\"\"\n",
    "    A partir do caminho do arquivo bronze em Parquet, cria um dataframe.\n",
    "    Realiza a transformação das colunas de string para tipos de dados relevantes.\n",
    "    Salva na camada Silver Transacional (Parquet) e Analítica (Tabelas de Banco de Dados).\n",
    "    \"\"\"\n",
    "    \n",
    "    df_calendar = spark.read.parquet(path_bronze_calendar)\n",
    "\n",
    "    df_calendar = (df_calendar\n",
    "                   .withColumn(\"listing_id\", col(\"listing_id\").cast(LongType()))\n",
    "                   .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "                   .withColumn(\"available\", \n",
    "                               when(col(\"available\") == \"t\", True).\n",
    "                               when(col(\"available\") == \"f\", False).\n",
    "                               otherwise(None))\n",
    "                   .withColumn(\"price\", regexp_replace(col(\"price\"), \"\\\\$\", \"\"))\n",
    "                   .withColumn(\"price\", regexp_replace(col(\"price\"), \",\", \"\"))\n",
    "                   .withColumn(\"price\", col(\"price\").cast(\"float\"))\n",
    "                   .withColumn(\"minimum_nights\", col(\"minimum_nights\").cast(\"integer\"))\n",
    "                   .withColumn(\"maximum_nights\", col(\"maximum_nights\").cast(\"integer\")))\n",
    "    \n",
    "    # Silver Transacional\n",
    "    df_calendar.write.mode(\"overwrite\").parquet(path_silver_calendar)\n",
    "\n",
    "    # Silver Analítica\n",
    "    df_calendar.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{nome_db}.silver_calendar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3340e137-5698-4b47-95b7-7c6d92e407b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_silver_listings(path_bronze_listings, path_silver_listings):\n",
    "    \"\"\"\n",
    "    A partir do caminho do arquivo bronze em Parquet, cria um dataframe.\n",
    "    Realiza a transformação das colunas de string para tipos de dados relevantes.\n",
    "    Salva na camada Silver Transacional (Parquet) e Analítica (Tabelas de Banco de Dados).\n",
    "    \"\"\"\n",
    "\n",
    "    tratar_antes = [\"host_response_rate\", \"host_acceptance_rate\", \"price\"]\n",
    "\n",
    "    numericos_int = [\"host_id\", \"host_response_rate\", \"host_acceptance_rate\",\"host_listings_count\", \"host_total_listings_count\", \n",
    "                    \"accommodates\", \"bedrooms\", \"beds\", \"minimum_nights\", \"maximum_nights\", \"minimum_minimum_nights\", \n",
    "                    \"maximum_minimum_nights\", \"minimum_maximum_nights\", \"maximum_maximum_nights\", \"availability_30\", \n",
    "                    \"availability_60\", \"availability_90\", \"availability_365\", \"number_of_reviews\", \"number_of_reviews_ltm\", \n",
    "                    \"number_of_reviews_l30d\", \"calculated_host_listings_count\", \"calculated_host_listings_count_entire_homes\", \n",
    "                    \"calculated_host_listings_count_private_rooms\", \"calculated_host_listings_count_shared_rooms\"]\n",
    "\n",
    "    numericos_float = [\"latitude\", \"longitude\", \"bathrooms\", \"price\", \"minimum_nights_avg_ntm\", \"maximum_nights_avg_ntm\", \n",
    "                    \"review_scores_rating\", \"review_scores_accuracy\", \"review_scores_cleanliness\", \"review_scores_checkin\", \n",
    "                    \"review_scores_communication\", \"review_scores_location\", \"review_scores_value\", \"reviews_per_month\"]\n",
    "\n",
    "    numericos_long = [\"id\", \"scrape_id\"]\n",
    "\n",
    "    colunas_data = [\"last_scraped\", \"host_since\", \"calendar_last_scraped\", \"first_review\", \"last_review\"]\n",
    "\n",
    "    booleanos = [\"host_is_superhost\", \"host_has_profile_pic\", \"host_identity_verified\", \"has_availability\", \"instant_bookable\"]\n",
    "\n",
    "    listas = [\"host_verifications\", \"amenities\"]\n",
    "\n",
    "    df_listings = spark.read.parquet(path_bronze_listings)\n",
    "\n",
    "    # Removendo caracteres\n",
    "    for i in tratar_antes:\n",
    "        df_listings = (df_listings\n",
    "                       .withColumn(i, regexp_replace(col(i), \"\\\\$\", \"\"))\n",
    "                       .withColumn(i, regexp_replace(col(i), \",\", \"\"))\n",
    "                       .withColumn(i, regexp_replace(col(i), \" \", \"\"))\n",
    "                       .withColumn(i, regexp_replace(col(i), \"\\\\%\", \"\"))\n",
    "                       )\n",
    "        \n",
    "    # Tratamento colunas Integer\n",
    "    for i in numericos_int:\n",
    "        df_listings = df_listings.withColumn(i, col(i).cast(IntegerType()))\n",
    "\n",
    "    # Transformar em Float\n",
    "    for i in numericos_float:\n",
    "        df_listings = df_listings.withColumn(i, col(i).cast(FloatType()))\n",
    "\n",
    "    # Transformar em LongType\n",
    "    for i in numericos_long:\n",
    "        df_listings = df_listings.withColumn(i, col(i).cast(LongType()))\n",
    "\n",
    "    # Transformar em Data\n",
    "    for i in colunas_data:\n",
    "        df_listings = df_listings.withColumn(i, to_date(i, \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Transformar em Booleanos\n",
    "    for i in booleanos:\n",
    "        df_listings = (df_listings.withColumn(i, \n",
    "                                              when(col(i) == \"t\", True).\n",
    "                                              when(col(i) == \"f\", False).\n",
    "                                              otherwise(None)\n",
    "                                              ))\n",
    "    \n",
    "    for i in df_listings.columns:\n",
    "        df_listings = (df_listings.withColumn(i, when(col(i) == \"N/A\", None).otherwise(col(i))))\n",
    "    \n",
    "    # Silver Transacional\n",
    "    df_listings.write.mode(\"overwrite\").parquet(path_silver_listings)\n",
    "\n",
    "    # Silver Analítica\n",
    "    df_listings.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{nome_db}.silver_listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4f7afbc-c2e1-4f6c-81eb-e9c7cdb44549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_silver_reviews(path_bronze_reviews, path_silver_reviews):\n",
    "    \"\"\"\n",
    "    A partir do caminho do arquivo bronze em Parquet, cria um dataframe.\n",
    "    Realiza a transformação das colunas de string para tipos de dados relevantes.\n",
    "    Salva na camada Silver Transacional (Parquet) e Analítica (Tabelas de Banco de Dados).\n",
    "    \"\"\"\n",
    "    \n",
    "    df_reviews = spark.read.parquet(path_bronze_reviews)\n",
    "    \n",
    "    df_reviews = df_reviews.drop(\"adjusted_price\")\n",
    "    \n",
    "    df_reviews = (df_reviews\n",
    "                  .withColumn(\"listing_id\", col(\"listing_id\").cast(LongType()))\n",
    "                  .withColumn(\"id\", col(\"id\").cast(LongType()))\n",
    "                  .withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "                  .withColumn(\"reviewer_id\", col(\"reviewer_id\").cast(LongType())))\n",
    "    \n",
    "    # Silver Transacional\n",
    "    df_reviews.write.mode(\"overwrite\").parquet(path_silver_reviews)\n",
    "\n",
    "    # Silver Analítica\n",
    "    df_reviews.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{nome_db}.silver_reviews\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
